{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series forecast with tensorflow\n",
    "\n",
    "refs: \n",
    "* https://www.tensorflow.org/tutorials/structured_data/time_series\n",
    "* https://www.tensorflow.org/guide/data\n",
    "* very nice explanation about how LSTM unit works.How it forget or remember (memory)\n",
    "    * https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T22:27:02.873448Z",
     "start_time": "2019-10-10T22:26:58.187570Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "# enable eager execution . should be done in program startup\n",
    "# refs: 1.10 https://dongnanzhy.github.io/2019/01/06/dcgan/\n",
    "# refs: https://medium.com/coding-blocks/eager-execution-in-tensorflow-a-more-pythonic-way-of-building-models-e461810618c8\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import pathlib \n",
    "\n",
    "import datetime\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "\n",
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T22:27:02.998613Z",
     "start_time": "2019-10-10T22:27:02.875387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/sdb1/leandro/time_series_forecast_with_tensorflow/dev\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pwd\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* enable eager executuion\n",
    "\n",
    "refs: https://medium.com/coding-blocks/eager-execution-in-tensorflow-a-more-pythonic-way-of-building-models-e461810618c8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T17:13:47.495076Z",
     "start_time": "2019-10-10T17:13:47.411092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=0, shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1., 0.],\n",
       "       [0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## enable eager execution\n",
    "\n",
    "# tf 2.0 eager are enable by default. But not in 1.0\n",
    "tf.executing_eagerly()\n",
    "\n",
    "tf.constant([[1,0],[0,1]],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T22:27:05.319798Z",
     "start_time": "2019-10-10T22:27:05.307055Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T19:18:39.519398Z",
     "start_time": "2019-10-10T19:18:39.366684Z"
    }
   },
   "outputs": [],
   "source": [
    "# creates log dir\n",
    "#!mkdir -p logs\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rvf ./logs/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Setting conda enviromewnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "```sh\n",
    "# Create env \n",
    "conda create -n tensorflow-timeseries\n",
    "\n",
    "# activate env\n",
    "conda activate tensorflow-timeseries\n",
    "\n",
    "# install tensorflow 2.0 \n",
    "pip install tensorflow # now is default in pip\n",
    "python -c \"import tensorflow as tf; print(tf.__version__)\"\n",
    "\n",
    "# enable the env in jupyter notebook\n",
    "conda install ipykernel\n",
    "\n",
    "# normally jupyter will automatic detect alll conda envs. INcase it fail you can add\n",
    "python -m ipykernel install --user --name=tensorflow-timeseries\n",
    "\n",
    "# install tensorboard\n",
    "pip install -q tf-nightly-2.0-preview\n",
    "\n",
    "# deactivate \n",
    "conda deactivate\n",
    "\n",
    "# generate yaml file\n",
    "conda env export > tensorflow-timeseries.yaml \n",
    "\n",
    "# create env from yaml\n",
    "conda env create -f tensorflow-timeseries.yaml \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T23:33:29.480438Z",
     "start_time": "2019-10-09T23:33:21.117846Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/leandroohf/anaconda3:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "_ipyw_jlab_nb_ext_conf    0.1.0                    py37_0  \r\n",
      "_libgcc_mutex             0.1                        main  \r\n",
      "alabaster                 0.7.12                   py37_0  \r\n",
      "anaconda                  2019.10                  py37_0  \r\n",
      "anaconda-client           1.7.2                    py37_0  \r\n",
      "anaconda-navigator        1.9.7                    py37_0  \r\n",
      "anaconda-project          0.8.3                      py_0  \r\n",
      "asn1crypto                1.0.1                    py37_0  \r\n",
      "astroid                   2.3.1                    py37_0  \r\n",
      "astropy                   3.2.2            py37h7b6447c_0  \r\n",
      "atomicwrites              1.3.0                    py37_1  \r\n",
      "attrs                     19.2.0                     py_0  \r\n",
      "babel                     2.7.0                      py_0  \r\n",
      "backcall                  0.1.0                    py37_0  \r\n",
      "backports                 1.0                        py_2  \r\n",
      "backports.functools_lru_cache 1.5                        py_2  \r\n",
      "backports.os              0.1.1                    py37_0  \r\n",
      "backports.shutil_get_terminal_size 1.0.0                    py37_2  \r\n",
      "backports.tempfile        1.0                        py_1  \r\n",
      "backports.weakref         1.0.post1                  py_1  \r\n",
      "beautifulsoup4            4.8.0                    py37_0  \r\n",
      "bitarray                  1.0.1            py37h7b6447c_0  \r\n",
      "bkcharts                  0.2                      py37_0  \r\n",
      "blas                      1.0                         mkl  \r\n",
      "bleach                    3.1.0                    py37_0  \r\n",
      "blosc                     1.16.3               hd408876_0  \r\n",
      "bokeh                     1.3.4                    py37_0  \r\n",
      "boto                      2.49.0                   py37_0  \r\n",
      "bottleneck                1.2.1            py37h035aef0_1  \r\n",
      "bzip2                     1.0.8                h7b6447c_0  \r\n",
      "ca-certificates           2019.8.28                     0    anaconda\r\n",
      "cairo                     1.14.12              h8948797_3  \r\n",
      "certifi                   2019.9.11                py37_0    conda-forge\r\n",
      "cffi                      1.12.3           py37h2e261b9_0  \r\n",
      "chardet                   3.0.4                 py37_1003  \r\n",
      "click                     7.0                      py37_0  \r\n",
      "cloudpickle               1.2.2                      py_0  \r\n",
      "clyent                    1.2.2                    py37_1  \r\n",
      "colorama                  0.4.1                    py37_0  \r\n",
      "conda                     4.7.12                   py37_0    conda-forge\r\n",
      "conda-build               3.18.9                   py37_3  \r\n",
      "conda-env                 2.6.0                         1  \r\n",
      "conda-package-handling    1.6.0            py37h7b6447c_0  \r\n",
      "conda-verify              3.4.2                      py_1  \r\n",
      "contextlib2               0.6.0                      py_0  \r\n",
      "cryptography              2.7              py37h1ba5d50_0  \r\n",
      "curl                      7.65.3               hbc83047_0  \r\n",
      "cycler                    0.10.0                   py37_0  \r\n",
      "cython                    0.29.13          py37he6710b0_0  \r\n",
      "cytoolz                   0.10.0           py37h7b6447c_0  \r\n",
      "dask                      2.5.2                      py_0  \r\n",
      "dask-core                 2.5.2                      py_0  \r\n",
      "dbus                      1.13.6               h746ee38_0  \r\n",
      "decorator                 4.4.0                    py37_1  \r\n",
      "defusedxml                0.6.0                      py_0  \r\n",
      "distributed               2.5.2                      py_0  \r\n",
      "docutils                  0.15.2                   py37_0  \r\n",
      "entrypoints               0.3                      py37_0  \r\n",
      "et_xmlfile                1.0.1                    py37_0  \r\n",
      "expat                     2.2.6                he6710b0_0  \r\n",
      "fastcache                 1.1.0            py37h7b6447c_0  \r\n",
      "filelock                  3.0.12                     py_0  \r\n",
      "flask                     1.1.1                      py_0  \r\n",
      "fontconfig                2.13.0               h9420a91_0  \r\n",
      "freetype                  2.9.1                h8a8886c_1  \r\n",
      "fribidi                   1.0.5                h7b6447c_0  \r\n",
      "fsspec                    0.5.2                      py_0  \r\n",
      "future                    0.17.1                   py37_0  \r\n",
      "get_terminal_size         1.0.0                haa9412d_0  \r\n",
      "gevent                    1.4.0            py37h7b6447c_0  \r\n",
      "glib                      2.56.2               hd408876_0  \r\n",
      "glob2                     0.7                        py_0  \r\n",
      "gmp                       6.1.2                h6c8ec71_1  \r\n",
      "gmpy2                     2.0.8            py37h10f8cd9_2  \r\n",
      "graphite2                 1.3.13               h23475e2_0  \r\n",
      "greenlet                  0.4.15           py37h7b6447c_0  \r\n",
      "gst-plugins-base          1.14.0               hbbd80ab_1  \r\n",
      "gstreamer                 1.14.0               hb453b48_1  \r\n",
      "h5py                      2.9.0            py37h7918eee_0  \r\n",
      "harfbuzz                  1.8.8                hffaf4a1_0  \r\n",
      "hdf5                      1.10.4               hb1b8bf9_0  \r\n",
      "heapdict                  1.0.1                      py_0  \r\n",
      "html5lib                  1.0.1                    py37_0  \r\n",
      "icu                       58.2                 h9c2bf20_1  \r\n",
      "idna                      2.8                      py37_0  \r\n",
      "imageio                   2.6.0                    py37_0  \r\n",
      "imagesize                 1.1.0                    py37_0  \r\n",
      "importlib_metadata        0.23                     py37_0  \r\n",
      "intel-openmp              2019.4                      243  \r\n",
      "ipykernel                 5.1.2            py37h39e3cac_0  \r\n",
      "ipython                   7.8.0            py37h39e3cac_0  \r\n",
      "ipython-sql               0.3.9                 py37_1000    conda-forge\r\n",
      "ipython_genutils          0.2.0                    py37_0  \r\n",
      "ipywidgets                7.5.1                      py_0  \r\n",
      "isort                     4.3.21                   py37_0  \r\n",
      "itsdangerous              1.1.0                    py37_0  \r\n",
      "jbig                      2.1                  hdba287a_0  \r\n",
      "jdcal                     1.4.1                      py_0  \r\n",
      "jedi                      0.15.1                   py37_0  \r\n",
      "jeepney                   0.4.1                      py_0  \r\n",
      "jinja2                    2.10.3                     py_0  \r\n",
      "joblib                    0.13.2                   py37_0  \r\n",
      "jpeg                      9b                   h024ee3a_2  \r\n",
      "json5                     0.8.5                      py_0  \r\n",
      "jsonschema                3.0.2                    py37_0  \r\n",
      "jupyter                   1.0.0                    py37_7  \r\n",
      "jupyter_client            5.3.3                    py37_1  \r\n",
      "jupyter_console           6.0.0                    py37_0  \r\n",
      "jupyter_contrib_core      0.3.3                      py_2    conda-forge\r\n",
      "jupyter_contrib_nbextensions 0.5.1                    py37_0    conda-forge\r\n",
      "jupyter_core              4.5.0                      py_0  \r\n",
      "jupyter_highlight_selected_word 0.2.0                 py37_1000    conda-forge\r\n",
      "jupyter_latex_envs        1.4.4                 py37_1000    conda-forge\r\n",
      "jupyter_nbextensions_configurator 0.4.1                    py37_0    conda-forge\r\n",
      "jupyterlab                1.1.4              pyhf63ae98_0  \r\n",
      "jupyterlab_server         1.0.6                      py_0  \r\n",
      "keyring                   18.0.0                   py37_0  \r\n",
      "kiwisolver                1.1.0            py37he6710b0_0  \r\n",
      "krb5                      1.16.1               h173b8e3_7  \r\n",
      "lazy-object-proxy         1.4.2            py37h7b6447c_0  \r\n",
      "libarchive                3.3.3                h5d8350f_5  \r\n",
      "libcurl                   7.65.3               h20c2e04_0  \r\n",
      "libedit                   3.1.20181209         hc058e9b_0  \r\n",
      "libffi                    3.2.1                hd88cf55_4  \r\n",
      "libgcc-ng                 9.1.0                hdf63c60_0  \r\n",
      "libgfortran-ng            7.3.0                hdf63c60_0  \r\n",
      "liblief                   0.9.0                h7725739_2  \r\n",
      "libpng                    1.6.37               hbc83047_0  \r\n",
      "libpq                     11.2                 h20c2e04_0    anaconda\r\n",
      "libsodium                 1.0.16               h1bed415_0  \r\n",
      "libssh2                   1.8.2                h1ba5d50_0  \r\n",
      "libstdcxx-ng              9.1.0                hdf63c60_0  \r\n",
      "libtiff                   4.0.10               h2733197_2  \r\n",
      "libtool                   2.4.6                h7b6447c_5  \r\n",
      "libuuid                   1.0.3                h1bed415_2  \r\n",
      "libxcb                    1.13                 h1bed415_1  \r\n",
      "libxml2                   2.9.9                hea5a465_1  \r\n",
      "libxslt                   1.1.33               h7d1a2b0_0  \r\n",
      "llvmlite                  0.29.0           py37hd408876_0  \r\n",
      "locket                    0.2.0                    py37_1  \r\n",
      "lxml                      4.4.1            py37hefd8a0e_0  \r\n",
      "lz4-c                     1.8.1.2              h14c3975_0  \r\n",
      "lzo                       2.10                 h49e0be7_2  \r\n",
      "markupsafe                1.1.1            py37h7b6447c_0  \r\n",
      "matplotlib                3.1.1            py37h5429711_0  \r\n",
      "mccabe                    0.6.1                    py37_1  \r\n",
      "mistune                   0.8.4            py37h7b6447c_0  \r\n",
      "mkl                       2019.4                      243  \r\n",
      "mkl-service               2.3.0            py37he904b0f_0  \r\n",
      "mkl_fft                   1.0.14           py37ha843d7b_0  \r\n",
      "mkl_random                1.1.0            py37hd6b4f25_0  \r\n",
      "mock                      3.0.5                    py37_0  \r\n",
      "more-itertools            7.2.0                    py37_0  \r\n",
      "mpc                       1.1.0                h10f8cd9_1  \r\n",
      "mpfr                      4.0.1                hdf1c602_3  \r\n",
      "mpmath                    1.1.0                    py37_0  \r\n",
      "msgpack-python            0.6.1            py37hfd86e86_1  \r\n",
      "multipledispatch          0.6.0                    py37_0  \r\n",
      "navigator-updater         0.2.1                    py37_0  \r\n",
      "nbconvert                 5.6.0                    py37_1  \r\n",
      "nbformat                  4.4.0                    py37_0  \r\n",
      "ncurses                   6.1                  he6710b0_1  \r\n",
      "networkx                  2.3                        py_0  \r\n",
      "nltk                      3.4.5                    py37_0  \r\n",
      "nose                      1.3.7                    py37_2  \r\n",
      "notebook                  6.0.1                    py37_0  \r\n",
      "numba                     0.45.1           py37h962f231_0  \r\n",
      "numexpr                   2.7.0            py37h9e4a6bb_0  \r\n",
      "numpy                     1.17.2           py37haad9e8e_0  \r\n",
      "numpy-base                1.17.2           py37hde5b4d6_0  \r\n",
      "numpydoc                  0.9.1                      py_0  \r\n",
      "olefile                   0.46                     py37_0  \r\n",
      "openpyxl                  3.0.0                      py_0  \r\n",
      "openssl                   1.1.1d               h7b6447c_2    anaconda\r\n",
      "packaging                 19.2                       py_0  \r\n",
      "pandas                    0.25.1           py37he6710b0_0  \r\n",
      "pandoc                    2.2.3.2                       0  \r\n",
      "pandocfilters             1.4.2                    py37_1  \r\n",
      "pango                     1.42.4               h049681c_0  \r\n",
      "parso                     0.5.1                      py_0  \r\n",
      "partd                     1.0.0                      py_0  \r\n",
      "patchelf                  0.9                  he6710b0_3  \r\n",
      "path.py                   12.0.1                     py_0  \r\n",
      "pathlib2                  2.3.5                    py37_0  \r\n",
      "patsy                     0.5.1                    py37_0  \r\n",
      "pcre                      8.43                 he6710b0_0  \r\n",
      "pep8                      1.7.1                    py37_0  \r\n",
      "pexpect                   4.7.0                    py37_0  \r\n",
      "pgspecial                 1.11.3                     py_0    conda-forge\r\n",
      "pickleshare               0.7.5                    py37_0  \r\n",
      "pillow                    6.2.0            py37h34e0f95_0  \r\n",
      "pip                       19.2.3                   py37_0  \r\n",
      "pixman                    0.38.0               h7b6447c_0  \r\n",
      "pkginfo                   1.5.0.1                  py37_0  \r\n",
      "pluggy                    0.13.0                   py37_0  \r\n",
      "ply                       3.11                     py37_0  \r\n",
      "prettytable               0.7.2                      py_3    conda-forge\r\n",
      "prometheus_client         0.7.1                      py_0  \r\n",
      "prompt_toolkit            2.0.10                     py_0  \r\n",
      "psutil                    5.6.3            py37h7b6447c_0  \r\n",
      "psycopg2                  2.8.3            py37h1ba5d50_0    anaconda\r\n",
      "ptyprocess                0.6.0                    py37_0  \r\n",
      "py                        1.8.0                    py37_0  \r\n",
      "py-lief                   0.9.0            py37h7725739_2  \r\n",
      "pycodestyle               2.5.0                    py37_0  \r\n",
      "pycosat                   0.6.3            py37h14c3975_0  \r\n",
      "pycparser                 2.19                     py37_0  \r\n",
      "pycrypto                  2.6.1            py37h14c3975_9  \r\n",
      "pycurl                    7.43.0.3         py37h1ba5d50_0  \r\n",
      "pyflakes                  2.1.1                    py37_0  \r\n",
      "pygments                  2.4.2                      py_0  \r\n",
      "pylint                    2.4.2                    py37_0  \r\n",
      "pyodbc                    4.0.27           py37he6710b0_0  \r\n",
      "pyopenssl                 19.0.0                   py37_0  \r\n",
      "pyparsing                 2.4.2                      py_0  \r\n",
      "pyqt                      5.9.2            py37h05f1152_2  \r\n",
      "pyrsistent                0.15.4           py37h7b6447c_0  \r\n",
      "pysocks                   1.7.1                    py37_0  \r\n",
      "pytables                  3.5.2            py37h71ec239_1  \r\n",
      "pytest                    5.2.1                    py37_0  \r\n",
      "pytest-arraydiff          0.3              py37h39e3cac_0  \r\n",
      "pytest-astropy            0.5.0                    py37_0  \r\n",
      "pytest-doctestplus        0.4.0                      py_0  \r\n",
      "pytest-openfiles          0.4.0                      py_0  \r\n",
      "pytest-remotedata         0.3.2                    py37_0  \r\n",
      "python                    3.7.4                h265db76_1  \r\n",
      "python-dateutil           2.8.0                    py37_0  \r\n",
      "python-libarchive-c       2.8                     py37_13  \r\n",
      "pytz                      2019.3                     py_0  \r\n",
      "pywavelets                1.0.3            py37hdd07704_1  \r\n",
      "pyyaml                    5.1.2            py37h7b6447c_0  \r\n",
      "pyzmq                     18.1.0           py37he6710b0_0  \r\n",
      "qt                        5.9.7                h5867ecd_1  \r\n",
      "qtawesome                 0.6.0                      py_0  \r\n",
      "qtconsole                 4.5.5                      py_0  \r\n",
      "qtpy                      1.9.0                      py_0  \r\n",
      "readline                  7.0                  h7b6447c_5  \r\n",
      "requests                  2.22.0                   py37_0  \r\n",
      "ripgrep                   0.10.0               hc07d326_0  \r\n",
      "rope                      0.14.0                     py_0  \r\n",
      "ruamel_yaml               0.15.46          py37h14c3975_0  \r\n",
      "scikit-image              0.15.0           py37he6710b0_0  \r\n",
      "scikit-learn              0.21.3           py37hd81dba3_0  \r\n",
      "scipy                     1.3.1            py37h7c811a0_0  \r\n",
      "seaborn                   0.9.0                    py37_0  \r\n",
      "secretstorage             3.1.1                    py37_0  \r\n",
      "send2trash                1.5.0                    py37_0  \r\n",
      "setuptools                41.4.0                   py37_0  \r\n",
      "simplegeneric             0.8.1                    py37_2  \r\n",
      "singledispatch            3.4.0.3                  py37_0  \r\n",
      "sip                       4.19.8           py37hf484d3e_0  \r\n",
      "six                       1.12.0                   py37_0  \r\n",
      "snappy                    1.1.7                hbae5bb6_3  \r\n",
      "snowballstemmer           2.0.0                      py_0  \r\n",
      "sortedcollections         1.1.2                    py37_0  \r\n",
      "sortedcontainers          2.1.0                    py37_0  \r\n",
      "soupsieve                 1.9.3                    py37_0  \r\n",
      "sphinx                    2.2.0                      py_0  \r\n",
      "sphinxcontrib             1.0                      py37_1  \r\n",
      "sphinxcontrib-applehelp   1.0.1                      py_0  \r\n",
      "sphinxcontrib-devhelp     1.0.1                      py_0  \r\n",
      "sphinxcontrib-htmlhelp    1.0.2                      py_0  \r\n",
      "sphinxcontrib-jsmath      1.0.1                      py_0  \r\n",
      "sphinxcontrib-qthelp      1.0.2                      py_0  \r\n",
      "sphinxcontrib-serializinghtml 1.1.3                      py_0  \r\n",
      "sphinxcontrib-websupport  1.1.2                      py_0  \r\n",
      "spyder                    3.3.6                    py37_0  \r\n",
      "spyder-kernels            0.5.2                    py37_0  \r\n",
      "sqlalchemy                1.3.9            py37h7b6447c_0  \r\n",
      "sqlite                    3.30.0               h7b6447c_0  \r\n",
      "sqlparse                  0.3.0                      py_0    conda-forge\r\n",
      "statsmodels               0.10.1           py37hdd07704_0  \r\n",
      "sympy                     1.4                      py37_0  \r\n",
      "tbb                       2019.4               hfd86e86_0  \r\n",
      "tblib                     1.4.0                      py_0  \r\n",
      "terminado                 0.8.2                    py37_0  \r\n",
      "testpath                  0.4.2                    py37_0  \r\n",
      "tk                        8.6.8                hbc83047_0  \r\n",
      "toolz                     0.10.0                     py_0  \r\n",
      "tornado                   6.0.3            py37h7b6447c_0  \r\n",
      "tqdm                      4.36.1                     py_0  \r\n",
      "traitlets                 4.3.3                    py37_0  \r\n",
      "unicodecsv                0.14.1                   py37_0  \r\n",
      "unixodbc                  2.3.7                h14c3975_0  \r\n",
      "urllib3                   1.24.2                   py37_0  \r\n",
      "wcwidth                   0.1.7                    py37_0  \r\n",
      "webencodings              0.5.1                    py37_1  \r\n",
      "werkzeug                  0.16.0                     py_0  \r\n",
      "wheel                     0.33.6                   py37_0  \r\n",
      "widgetsnbextension        3.5.1                    py37_0  \r\n",
      "wrapt                     1.11.2           py37h7b6447c_0  \r\n",
      "wurlitzer                 1.0.3                    py37_0  \r\n",
      "xlrd                      1.2.0                    py37_0  \r\n",
      "xlsxwriter                1.2.1                      py_0  \r\n",
      "xlwt                      1.3.0                    py37_0  \r\n",
      "xz                        5.2.4                h14c3975_4  \r\n",
      "yaml                      0.1.7                had09818_2  \r\n",
      "zeromq                    4.3.1                he6710b0_3  \r\n",
      "zict                      1.0.0                      py_0  \r\n",
      "zipp                      0.6.0                      py_0  \r\n",
      "zlib                      1.2.11               h7b6447c_3  \r\n",
      "zstd                      1.3.7                h0b5b093_0  \r\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The weather dataset\n",
    "\n",
    "This dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. These were collected **every 10 minutes**, beginning in 2003. For efficiency, you will use only the data collected between 2009 and 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T20:37:30.682325Z",
     "start_time": "2019-10-09T20:37:29.580013Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\n",
      "13574144/13568290 [==============================] - 1s 0us/step\n",
      "/home/leandroohf/.keras/datasets/jena_climate_2009_2016.csv\n",
      ".zip\n",
      "'/home/leandroohf/.keras/datasets/jena_climate_2009_2016.csv.zip' -> '../data/jena_climate_2009_2016.csv.zip'\n",
      "Archive:  ../data/jena_climate_2009_2016.csv.zip\n",
      "  inflating: ../data/jena_climate_2009_2016.csv  \n",
      "/bin/sh: 1: tree: not found\n"
     ]
    }
   ],
   "source": [
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True)\n",
    "\n",
    "csv_path, ext = os.path.splitext(zip_path)\n",
    "\n",
    "print(csv_path)\n",
    "print(ext)\n",
    "\n",
    "!mkdir -p ../data\n",
    "!cp -v {zip_path} ../data\n",
    "!unzip -o ../data/jena_climate_2009_2016.csv.zip -d ../data\n",
    "!tree -n 1 ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T22:27:11.267932Z",
     "start_time": "2019-10-10T22:27:09.771424Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Time</th>\n",
       "      <th>p (mbar)</th>\n",
       "      <th>T (degC)</th>\n",
       "      <th>Tpot (K)</th>\n",
       "      <th>Tdew (degC)</th>\n",
       "      <th>rh (%)</th>\n",
       "      <th>VPmax (mbar)</th>\n",
       "      <th>VPact (mbar)</th>\n",
       "      <th>VPdef (mbar)</th>\n",
       "      <th>sh (g/kg)</th>\n",
       "      <th>H2OC (mmol/mol)</th>\n",
       "      <th>rho (g/m**3)</th>\n",
       "      <th>wv (m/s)</th>\n",
       "      <th>max. wv (m/s)</th>\n",
       "      <th>wd (deg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.01.2009 00:10:00</td>\n",
       "      <td>996.52</td>\n",
       "      <td>-8.02</td>\n",
       "      <td>265.40</td>\n",
       "      <td>-8.90</td>\n",
       "      <td>93.3</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.12</td>\n",
       "      <td>1307.75</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.75</td>\n",
       "      <td>152.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.01.2009 00:20:00</td>\n",
       "      <td>996.57</td>\n",
       "      <td>-8.41</td>\n",
       "      <td>265.01</td>\n",
       "      <td>-9.28</td>\n",
       "      <td>93.4</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.02</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.89</td>\n",
       "      <td>3.03</td>\n",
       "      <td>1309.80</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.50</td>\n",
       "      <td>136.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.01.2009 00:30:00</td>\n",
       "      <td>996.53</td>\n",
       "      <td>-8.51</td>\n",
       "      <td>264.91</td>\n",
       "      <td>-9.31</td>\n",
       "      <td>93.9</td>\n",
       "      <td>3.21</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1310.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.63</td>\n",
       "      <td>171.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.01.2009 00:40:00</td>\n",
       "      <td>996.51</td>\n",
       "      <td>-8.31</td>\n",
       "      <td>265.12</td>\n",
       "      <td>-9.07</td>\n",
       "      <td>94.2</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1309.19</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.50</td>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.01.2009 00:50:00</td>\n",
       "      <td>996.51</td>\n",
       "      <td>-8.27</td>\n",
       "      <td>265.15</td>\n",
       "      <td>-9.04</td>\n",
       "      <td>94.1</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.09</td>\n",
       "      <td>1309.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.63</td>\n",
       "      <td>214.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date Time  p (mbar)  T (degC)  Tpot (K)  Tdew (degC)  rh (%)  \\\n",
       "0  01.01.2009 00:10:00    996.52     -8.02    265.40        -8.90    93.3   \n",
       "1  01.01.2009 00:20:00    996.57     -8.41    265.01        -9.28    93.4   \n",
       "2  01.01.2009 00:30:00    996.53     -8.51    264.91        -9.31    93.9   \n",
       "3  01.01.2009 00:40:00    996.51     -8.31    265.12        -9.07    94.2   \n",
       "4  01.01.2009 00:50:00    996.51     -8.27    265.15        -9.04    94.1   \n",
       "\n",
       "   VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  \\\n",
       "0          3.33          3.11          0.22       1.94             3.12   \n",
       "1          3.23          3.02          0.21       1.89             3.03   \n",
       "2          3.21          3.01          0.20       1.88             3.02   \n",
       "3          3.26          3.07          0.19       1.92             3.08   \n",
       "4          3.27          3.08          0.19       1.92             3.09   \n",
       "\n",
       "   rho (g/m**3)  wv (m/s)  max. wv (m/s)  wd (deg)  \n",
       "0       1307.75      1.03           1.75     152.3  \n",
       "1       1309.80      0.72           1.50     136.1  \n",
       "2       1310.24      0.19           0.63     171.6  \n",
       "3       1309.19      0.34           0.50     198.0  \n",
       "4       1309.00      0.32           0.63     214.3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = '../data/jena_climate_2009_2016.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The func bellow returns a window of obsrvations data \n",
    "* history_size: is the size of the past window of information\n",
    "* target_size is how far in the future does the model need to learn to predict\n",
    "\n",
    "For instance: 5 days obeservations is a window with hitory_size = 6720 (2x6*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T21:29:05.450528Z",
     "start_time": "2019-10-10T21:29:05.443020Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        \n",
    "        indices = range(i-history_size, i)\n",
    "        \n",
    "        # Reshape data from (history_size,) to (history_size, 1)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        labels.append(dataset[i+target_size])\n",
    "        \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The func belows plot thw train data and the predict point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T22:31:04.586755Z",
     "start_time": "2019-10-10T22:31:04.575824Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_time_steps(length):\n",
    "    time_steps = []\n",
    "    for i in range(-length, 0, 1):\n",
    "        time_steps.append(i)\n",
    "        \n",
    "    return time_steps\n",
    "\n",
    "def show_plot(plot_data, delta, title):\n",
    "    \n",
    "    labels = ['History', 'True Future', 'Model Prediction']\n",
    "    marker = ['.-', 'rx', 'go']\n",
    "    time_steps = create_time_steps(plot_data[0].shape[0])\n",
    "    \n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, x in enumerate(plot_data):\n",
    "        \n",
    "        if i:\n",
    "            \n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10,\n",
    "               label=labels[i])\n",
    "        else:\n",
    "            \n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future+5)*2])\n",
    "    plt.xlabel('Time-Step')\n",
    "            \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " Split data in train and validation dataset\n",
    " * train 300k observations\n",
    " * validation: ~2100 (remains)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T22:27:16.513446Z",
     "start_time": "2019-10-10T22:27:16.506849Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 300000\n",
    "\n",
    "#tf.set_random_seed(13)\n",
    "tf.random.set_seed(13) # tf core 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast a univariate time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T18:16:25.997311Z",
     "start_time": "2019-10-10T18:16:25.438448Z"
    }
   },
   "outputs": [],
   "source": [
    "uni_data_df = df['T (degC)']\n",
    "uni_data_df.index = df['Date Time']\n",
    "uni_data_df.head()\n",
    "\n",
    "uni_data_df.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* normalize data for the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T18:16:29.322232Z",
     "start_time": "2019-10-10T18:16:29.308417Z"
    }
   },
   "outputs": [],
   "source": [
    "uni_data = uni_data_df.values\n",
    "uni_data\n",
    "\n",
    "uni_train_mean = uni_data[:TRAIN_SPLIT].mean()\n",
    "uni_train_std = uni_data[:TRAIN_SPLIT].std()\n",
    "\n",
    "uni_data = (uni_data-uni_train_mean)/uni_train_std\n",
    "uni_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use the last **20 recorded temperature observations**, and predict the temperature at the next time step (one observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T18:16:41.520014Z",
     "start_time": "2019-10-10T18:16:31.566339Z"
    }
   },
   "outputs": [],
   "source": [
    "univariate_past_history = 20\n",
    "univariate_future_target = 0\n",
    "\n",
    "x_train_uni, y_train_uni = univariate_data(uni_data, 0, TRAIN_SPLIT,\n",
    "                                           univariate_past_history,\n",
    "                                           univariate_future_target)\n",
    "\n",
    "x_val_uni, y_val_uni = univariate_data(uni_data, TRAIN_SPLIT, None,\n",
    "                                       univariate_past_history,\n",
    "                                       univariate_future_target)\n",
    "\n",
    "print ('Single window of past history')\n",
    "print(x_train_uni.shape)\n",
    "print (x_train_uni[0])\n",
    "print ('\\n Target temperature to predict')\n",
    "print(y_train_uni.shape)\n",
    "print (y_train_uni[0])\n",
    "\n",
    "print('validation dataset')\n",
    "print(x_val_uni.shape)\n",
    "print(y_val_uni.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T18:16:41.756407Z",
     "start_time": "2019-10-10T18:16:41.521818Z"
    }
   },
   "outputs": [],
   "source": [
    "# next time step prediction \n",
    "show_plot([x_train_uni[0], y_train_uni[0]], 0, 'Sample Example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T18:16:41.982316Z",
     "start_time": "2019-10-10T18:16:41.758048Z"
    }
   },
   "outputs": [],
   "source": [
    "# next time step prediction is mean compared to zero\n",
    "np.mean(x_train_uni[0])\n",
    "show_plot([x_train_uni[0], y_train_uni[0], np.mean(x_train_uni[0])], 0,\n",
    "           'Baseline Prediction Example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T17:19:47.482607Z",
     "start_time": "2019-10-10T17:19:47.475396Z"
    }
   },
   "source": [
    "### Understand first tf.data api\n",
    "\n",
    "refs: https://www.tensorflow.org/guide/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* tf.data API\n",
    "\n",
    "Tensorflow provide a pipeli API called tf.data that can provide methods to handle different type of data.  For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training. The pipeline for a text model might involve extracting symbols from raw text data, converting them to embedding identifiers with a lookup table, and batching together sequences of different lengths.\n",
    "\n",
    "\n",
    "The tf.data API supports a **variety of file formats so that you can process large datasets that do not fit in memory**\n",
    "\n",
    "* tf.data.Dataset\n",
    "\n",
    "tf.data.Dataset abstraction that represents a **sequence of elements**, in which each element consists of one or more components. For example, in an image pipeline, an element might be a single training example, with a pair of tensor components representing the image and its label.\n",
    "\n",
    "if your input data is stored in a file in the recommended TFRecord format, you can use **tf.data.TFRecordDataset()**\n",
    "\n",
    "    * tf.data.Dataset.element_spec property allows you to inspect the type of each element component\n",
    "\n",
    "\n",
    "See examples bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T17:36:21.225328Z",
     "start_time": "2019-10-10T17:36:20.622248Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example of create Dataset from memory \n",
    "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
    "dataset\n",
    "\n",
    "counter = 0\n",
    "for elem in dataset:\n",
    "    counter += 1\n",
    "    print(elem.numpy())\n",
    "    \n",
    "print('counter: {}'.format(counter))\n",
    "    \n",
    "rd = tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32)\n",
    "\n",
    "rd.shape\n",
    "\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10]))\n",
    "\n",
    "dataset1\n",
    "print()\n",
    "dataset1.element_spec\n",
    "\n",
    "counter = 0\n",
    "for elem in dataset1:\n",
    "    counter += 1\n",
    "    \n",
    "print('counter: {}'.format(counter))\n",
    "    \n",
    "\n",
    "print()\n",
    "# Load dataset\n",
    "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "images, labels = train\n",
    "images = images/255\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See examples bellow how to read from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T17:43:35.238045Z",
     "start_time": "2019-10-10T17:43:35.232479Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creates a dataset that reads all of the examples from two files.\n",
    "fsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\", \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")\n",
    "\n",
    "# Download examples\n",
    "flowers_root = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True)\n",
    "\n",
    "flowers_root = pathlib.Path(flowers_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T17:40:42.423869Z",
     "start_time": "2019-10-10T17:40:42.400181Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file ])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consuming a list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T17:49:48.178318Z",
     "start_time": "2019-10-10T17:49:48.032145Z"
    }
   },
   "outputs": [],
   "source": [
    "# see folders contents\n",
    "for item in flowers_root.glob(\"*\"):\n",
    "    print(item.name)\n",
    "\n",
    "print()\n",
    "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\n",
    "\n",
    "for f in list_ds.take(5):\n",
    "    print(f.numpy())\n",
    "    \n",
    "\n",
    "print()\n",
    "# Convert the file paths to (image, label) pairs:\n",
    "def process_path(file_path):\n",
    "    \n",
    "    parts = tf.strings.split(file_path, '/')\n",
    "    return tf.io.read_file(file_path), parts[-2]\n",
    "\n",
    "labeled_ds = list_ds.map(process_path)\n",
    "\n",
    "for image_raw, label_text in labeled_ds.take(2):\n",
    "    print(repr(image_raw.numpy()[:100]))\n",
    "    print()\n",
    "    print(label_text.numpy())\n",
    "    print('========== next image ===========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **tf.data.Dadaset.cache(filename='')**\n",
    "\n",
    "Caches the elements in this dataset. filename representing the name of a directory on the filesystem to use for caching elements in this Dataset. **If a filename is not provided, the dataset will be cached in memory.**\n",
    "\n",
    "* **tf.data.Dadaset.shuffle(buffer_size,seed=None,reshuffle_each_iteration=None)**\n",
    "\n",
    "Randomly shuffles the elements of this dataset. This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. **For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.**\n",
    "\n",
    "* **tf.data.Dadaset.batch(batch_size, drop_remainder=False)**\n",
    "\n",
    "Combines consecutive elements of this dataset into batches. (Group of elemenst or sequences)\n",
    "\n",
    "* **tf.data.Dadaset.batch(count=None)**\n",
    "\n",
    "Repeats this dataset count times. The default behavior (if count is None or -1) is for the dataset be repeated indefinitely.\n",
    "\n",
    "\n",
    "**Let's shuffle, batch, and cache the train dataset.**\n",
    "\n",
    "\n",
    "<img src=\"../fig/tf_data_shuffle.png\" width=\"500\" align=\"left\"/> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T18:17:30.930715Z",
     "start_time": "2019-10-10T18:17:30.846775Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "x_train_uni.shape\n",
    "y_train_uni.shape \n",
    "\n",
    "print('#batches: {}'.format(int(x_train_uni.shape[0]/BATCH_SIZE)))\n",
    "\n",
    "train_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))\n",
    "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))\n",
    "val_univariate = val_univariate.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "type(train_univariate)\n",
    "train_univariate\n",
    "train_univariate.list_files\n",
    "\n",
    "\n",
    "print('Train dataset: Understanding batch data')\n",
    "# Understand the dim of the data\n",
    "for x, y  in train_univariate.take(1):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "print()\n",
    "print('Val dataset: Understanding batch data')\n",
    "# Understand the dim of the data\n",
    "for x, y  in val_univariate.take(1):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "\n",
    "# for x, y in val_univariate.take(1):\n",
    "#     print(simple_lstm_model.predict(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training  simple LSTM\n",
    "\n",
    "LSTM is a subtyep of RNN\n",
    "\n",
    "\n",
    "outputs of one neuron:  \n",
    "\n",
    "$\n",
    "c_t = f_t*c_{t-1}, x_t, h_{t-1})\n",
    "$\n",
    "\n",
    "$\n",
    "h_t = g(c_{t}, x_t)\n",
    "$\n",
    "\n",
    "<img src=\"../fig/lstm_neuron.png\" width=\"600\" align=\"left\"/> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T18:19:29.825155Z",
     "start_time": "2019-10-10T18:19:29.501245Z"
    }
   },
   "outputs": [],
   "source": [
    "log_dir=\"logs/fit1/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "# LSTM \n",
    "# #neuron in Layer: 8\n",
    "simple_lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(8, input_shape=x_train_uni.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# seting loss n optmizer\n",
    "#simple_lstm_model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=0.000001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "simple_lstm_model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "simple_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T23:52:13.983156Z",
     "start_time": "2019-10-09T23:52:13.465591Z"
    }
   },
   "outputs": [],
   "source": [
    "for x, y in val_univariate.take(1):\n",
    "    print(simple_lstm_model.predict(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T00:47:01.920546Z",
     "start_time": "2019-10-10T00:46:02.871159Z"
    }
   },
   "outputs": [],
   "source": [
    "EVALUATION_INTERVAL = 200\n",
    "EPOCHS = 30\n",
    "\n",
    "simple_lstm_model.fit(train_univariate, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate, validation_steps=50,\n",
    "                      callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T00:37:13.639424Z",
     "start_time": "2019-10-10T00:37:13.623512Z"
    }
   },
   "outputs": [],
   "source": [
    "time_step = 0\n",
    "for x, y in val_univariate.take(1):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "#simple_lstm_model.predict(x)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start tensorboard\n",
    "\n",
    "refs: https://www.tensorflow.org/tensorboard/get_started\n",
    "\n",
    "run in the command line \n",
    "\n",
    "```sh\n",
    "tensorboard --logdir logs/fit1\n",
    "```\n",
    "\n",
    "\n",
    "* The Scalars dashboard shows how the loss and metrics change with every epoch. You can use it to also track training speed, learning rate, and other scalar values.\n",
    "\n",
    "* The Graphs dashboard helps you visualize your model. In this case, the Keras graph of layers is shown which can help you ensure it is built correctly. \n",
    "\n",
    "* The Distributions and Histograms dashboards show the distribution of a Tensor over time. This can be useful to visualize weights and biases and verify that they are changing in an expected way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T00:37:15.642312Z",
     "start_time": "2019-10-10T00:37:14.070786Z"
    }
   },
   "outputs": [],
   "source": [
    "time_step = 0\n",
    "for x, y in val_univariate.take(5):\n",
    "    \n",
    "    plot = show_plot([x[0].numpy(), y[0].numpy(),\n",
    "                    simple_lstm_model.predict(x)[0]], time_step, 'Simple LSTM model')\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T23:35:47.870229Z",
     "start_time": "2019-10-09T23:35:44.829125Z"
    }
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast a multivariate time series: One step model\n",
    "\n",
    "720 observations that are sampled every hour.  Ex: 120 observation represent history of the last five days\n",
    "\n",
    "\n",
    "**The goal is to predict temprature based on 5 days observations of 3 features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T22:28:34.582739Z",
     "start_time": "2019-10-10T22:28:23.787020Z"
    }
   },
   "outputs": [],
   "source": [
    "features_considered = ['p (mbar)', 'T (degC)', 'rho (g/m**3)']\n",
    "\n",
    "features = df[features_considered]\n",
    "features.index = df['Date Time']\n",
    "features.head(3)\n",
    "\n",
    "print()\n",
    "features.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* normalize data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T22:28:34.599432Z",
     "start_time": "2019-10-10T22:28:34.584277Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = features.values\n",
    "\n",
    "data_mean = dataset.mean(axis=0)\n",
    "data_std = dataset.std(axis=0)\n",
    "\n",
    "dataset = (dataset-data_mean)/data_std\n",
    "\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single step setup, the model learns to predict a single point in the future based on some history provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T22:28:37.539905Z",
     "start_time": "2019-10-10T22:28:37.531373Z"
    }
   },
   "outputs": [],
   "source": [
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index): \n",
    "        \n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T19:19:44.305765Z",
     "start_time": "2019-10-10T19:19:20.992167Z"
    }
   },
   "outputs": [],
   "source": [
    "past_history = 720\n",
    "future_target = 72\n",
    "STEP = 6\n",
    "\n",
    "x_train_single, y_train_single = multivariate_data(dataset, dataset[:, 1], 0,\n",
    "                                                   TRAIN_SPLIT, past_history,\n",
    "                                                   future_target, STEP,\n",
    "                                                   single_step=True)\n",
    "\n",
    "x_val_single, y_val_single = multivariate_data(dataset, dataset[:, 1],\n",
    "                                               TRAIN_SPLIT, None, past_history,\n",
    "                                               future_target, STEP,\n",
    "                                               single_step=True)\n",
    "\n",
    "print ('Single window of past history : {}'.format(x_train_single[0].shape))\n",
    "y_train_single.shape\n",
    "print ('Single window of past history : {}'.format(x_val_single[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T19:19:45.224830Z",
     "start_time": "2019-10-10T19:19:44.307300Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))\n",
    "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
    "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "type(train_data_single)\n",
    "train_data_single\n",
    "\n",
    "print('Train dataset: Understanding batch data')\n",
    "# Understand the dim of the data\n",
    "for x, y  in train_data_single.take(1):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "print()\n",
    "print('Val dataset: Understanding batch data')\n",
    "# Understand the dim of the data\n",
    "for x, y  in val_data_single.take(1):\n",
    "    print(x.shape)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T19:19:45.651817Z",
     "start_time": "2019-10-10T19:19:45.226333Z"
    }
   },
   "outputs": [],
   "source": [
    "log_dir=\"logs/fit2/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "single_step_model = tf.keras.models.Sequential()\n",
    "\n",
    "# understand inputs shapes\n",
    "x_train_single.shape\n",
    "x_train_single.shape[-2:]\n",
    "\n",
    "# LSTM \n",
    "# # #neuron in Layer: 8\n",
    "single_step_model.add(tf.keras.layers.LSTM(32,\n",
    "                                           input_shape=x_train_single.shape[-2:]))\n",
    "\n",
    "single_step_model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "single_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')\n",
    "\n",
    "\n",
    "single_step_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T18:23:30.005468Z",
     "start_time": "2019-10-10T18:23:29.197054Z"
    }
   },
   "outputs": [],
   "source": [
    "# test sample predition\n",
    "for x, y in val_data_single.take(1):\n",
    "    print(single_step_model.predict(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* training\n",
    "\n",
    "**How to resatart from zero the model whithout change paramneters?** Should I delete the object or restart the notebook?\n",
    "\n",
    "\n",
    "\n",
    "**TODO:** fix it latter\n",
    "**This model has high variance problem*** If you trainn longer the validation loss is increasing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T21:05:50.537723Z",
     "start_time": "2019-10-10T21:02:39.139100Z"
    }
   },
   "outputs": [],
   "source": [
    "EVALUATION_INTERVAL = 200\n",
    "EPOCHS = 10\n",
    "\n",
    "single_step_history = single_step_model.fit(train_data_single, epochs=EPOCHS,\n",
    "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                            validation_data=val_data_single,\n",
    "                                            validation_steps=50,\n",
    "                                            callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " First models the validation loss start increasing indicating that we should try to reduce variance erro and not bias. **Because of that we reduce EPOCHS to 10**\n",
    " \n",
    " <img src=\"../fig/variance_error_increasing.png\" width=\"400\" align=\"left\"/>  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T22:27:46.481962Z",
     "start_time": "2019-10-10T22:27:46.475562Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_train_history(history, title):\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T21:20:40.237685Z",
     "start_time": "2019-10-10T21:20:40.066866Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_train_history(single_step_history,\n",
    "                   'Single Step Training and validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The model is given the history of three features over the past five days sampled every hour (120 data-points), since the goal is to predict the temperature, the plot only displays the past temperature. The prediction is made one day into the future (hence the gap between the history and prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for x, y in val_data_single.take(3):\n",
    "    \n",
    "    plot = show_plot([x[0][:, 1].numpy(), y[0].numpy(),\n",
    "                    single_step_model.predict(x)[0]], 12,\n",
    "                   'Single Step Prediction')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T21:05:50.543536Z",
     "start_time": "2019-10-10T21:05:50.539613Z"
    }
   },
   "source": [
    "## Forecast a multivariate time series: Multiple step model\n",
    "\n",
    "In a multi-step prediction model, given a past history, the model needs to learn to predict a range of future values. Thus, unlike a single step model, where only a single future point is predicted, a multi-step model **predict a sequence of the future.**\n",
    "\n",
    "The training data again consists of recordings **over the past five days sampled every hour**. However, here, the model needs to **learn to predict the temperature for the next 12 hours**. Since an obversation is taken every 10 minutes, the output is 72 predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T22:29:09.474590Z",
     "start_time": "2019-10-10T22:28:44.891538Z"
    }
   },
   "outputs": [],
   "source": [
    "past_history = 720\n",
    "future_target = 72\n",
    "STEP = 6\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "x_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 1], 0,\n",
    "                                                 TRAIN_SPLIT, past_history,\n",
    "                                                 future_target, STEP)\n",
    "\n",
    "x_val_multi, y_val_multi = multivariate_data(dataset, dataset[:, 1],\n",
    "                                             TRAIN_SPLIT, None, past_history,\n",
    "                                             future_target, STEP)\n",
    "\n",
    "\n",
    "print ('Single window of past history : {}'.format(x_train_multi[0].shape))\n",
    "print ('\\n Target temperature to predict : {}'.format(y_train_multi[0].shape))\n",
    "\n",
    "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T22:30:37.831474Z",
     "start_time": "2019-10-10T22:30:37.820761Z"
    }
   },
   "outputs": [],
   "source": [
    "def multi_step_plot(history, true_future, prediction):\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_in = create_time_steps(len(history))\n",
    "    num_out = len(true_future)\n",
    "\n",
    "    plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
    "    plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
    "           label='True Future')\n",
    "    \n",
    "    if prediction.any():\n",
    "        plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
    "             label='Predicted Future')\n",
    "        \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T22:31:16.503440Z",
     "start_time": "2019-10-10T22:31:15.965258Z"
    }
   },
   "outputs": [],
   "source": [
    "for x, y in train_data_multi.take(1):\n",
    "    multi_step_plot(x[0], y[0], np.array([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T22:31:22.384412Z",
     "start_time": "2019-10-10T22:31:20.207586Z"
    }
   },
   "outputs": [],
   "source": [
    "log_dir=\"logs/fit3/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "multi_step_model = tf.keras.models.Sequential()\n",
    "multi_step_model.add(tf.keras.layers.LSTM(32,\n",
    "                                          return_sequences=True,\n",
    "                                          input_shape=x_train_multi.shape[-2:]))\n",
    "\n",
    "multi_step_model.add(tf.keras.layers.LSTM(16, activation='relu'))\n",
    "\n",
    "multi_step_model.add(tf.keras.layers.Dense(72))\n",
    "\n",
    "\n",
    "# defaults valueas of thwe optmizer\n",
    "# __init__(\n",
    "#     learning_rate=0.001,\n",
    "#     rho=0.9,\n",
    "#     momentum=0.0,\n",
    "#     epsilon=1e-07,\n",
    "#     centered=False,\n",
    "#     name='RMSprop',\n",
    "# )\n",
    "\n",
    "rms_prop = tf.keras.optimizers.RMSprop(clipvalue=1.0, learning_rate=0.0001)\n",
    "\n",
    "print('Optmizer parmeters: ')\n",
    "print('lr: {}'.format(rms_prop.learning_rate))\n",
    "print('rho: {}'.format(rms_prop.rho))\n",
    "print('momentum: {}'.format(rms_prop.momentum))\n",
    "print('epsilon: {}'.format(rms_prop.epsilon))\n",
    "\n",
    "multi_step_model.compile(optimizer=rms_prop, loss='mae')\n",
    "\n",
    "print('input size:{}'.format(x_train_multi.shape[-2:]))\n",
    "\n",
    "multi_step_model.summary()\n",
    "\n",
    "# checking prediction output size\n",
    "for x, y in val_data_multi.take(1):\n",
    "    print (multi_step_model.predict(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T23:13:48.956159Z",
     "start_time": "2019-10-10T23:10:47.319774Z"
    }
   },
   "outputs": [],
   "source": [
    "EVALUATION_INTERVAL = 200\n",
    "EPOCHS = 5\n",
    "\n",
    "multi_step_history = multi_step_model.fit(train_data_multi, epochs=EPOCHS,\n",
    "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                          validation_data=val_data_multi,\n",
    "                                          validation_steps=50,\n",
    "                                          callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T23:18:39.774671Z",
     "start_time": "2019-10-10T23:18:39.590707Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T23:21:14.337307Z",
     "start_time": "2019-10-10T23:21:12.917759Z"
    }
   },
   "outputs": [],
   "source": [
    "for x, y in val_data_multi.take(3):\n",
    "    multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-timeseries",
   "language": "python",
   "name": "tensorflow-timeseries"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
