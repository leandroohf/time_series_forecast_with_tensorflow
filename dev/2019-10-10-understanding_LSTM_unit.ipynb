{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM unit\n",
    "\n",
    "\n",
    "\n",
    "refs: \n",
    "\n",
    "* https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "* https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714\n",
    "* https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "\n",
    "\n",
    "Consider trying to predict the last word in the text:\n",
    "\n",
    "> “I grew up in France …  blah, blah (many time) ...I speak fluent ______.”\n",
    "\n",
    "The answer is **French**. \n",
    "\n",
    "Recent information suggests that the next word is probably the name of a **language**, but if we want to narrow down which language, we need the context of **France**, from further back to predict **French**. \n",
    "\n",
    "In order to do that a RNN needs to remember **long-term depencies**. In theory RNN can do that but in practice due to gradient vanish and explodig gradient RNN does not have a **good long term memory**\n",
    "\n",
    "**Long Short Term Memory (LSTM)** networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies\n",
    "\n",
    "LSTMs are explicitly designed to avoid the long-term dependency problem metioned above. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\n",
    "\n",
    "<img src=\"../fig/lstm_neuron.png\" width=\"600\" align=\"left\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM unit works like pipes. The memory is water the valves are the activations functions. The memory flow (water) in the memory pipe are changed by 2 valves:\n",
    "\n",
    "* memory pipe\n",
    "* forget valve\n",
    "* new mwmory valve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The memory pipe\n",
    "\n",
    "The flow in this pipe determine how much of the unit memory will be mixed with the old memory to define the new memory state of the unit.   \n",
    "\n",
    "\n",
    "<img src=\"../fig/memory_pipe.png\" width=\"600\" align=\"left\"/> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Forget valve\n",
    "\n",
    "$\n",
    "f_t = \\sigma(W_f[c_{t-1}, h_{t-1}, X_t] + b_f)\n",
    "$\n",
    "\n",
    "\n",
    "If $f_t = 0$ the unit will copletely forget the previous memory state. On the other hand, ($f_t = 1.0$) the unit will rememember everything.  \n",
    "\n",
    "\n",
    "<img src=\"../fig/forget_valve.png\" width=\"600\" align=\"left\"/> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New memory valve\n",
    "\n",
    "\n",
    "$\n",
    "i_t = \\sigma(W_i[C_{t-1}, h_{t-1}, X_t] + b_i)\n",
    "$\n",
    "\n",
    "Like $f_t$, $i_t$ controsl how much of the new state memory will be used to mix with the old one.\n",
    "\n",
    "$\n",
    "G_t = tanh(W_g[C_{t-1}, h_{t-1}, X_t] + b_g)\n",
    "$\n",
    "\n",
    "The output of the unit is the memory state $c_t$\n",
    "\n",
    "\n",
    "$\n",
    "C_t = f_t*C_{t-1} + i_t*G_t\n",
    "$\n",
    "\n",
    "<img src=\"../fig/new_memory_valve.png\" width=\"600\" align=\"left\"/>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The output state of the unit\n",
    "\n",
    "$\n",
    "o_t = \\sigma(W_o[C_{t-1}, h_{t-1}, X_t] + b_o)\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "h_t = o_t * tanh(C_t)\n",
    "$\n",
    "\n",
    "The ouput state are:\n",
    "* $h_t$\n",
    "* $C_t$\n",
    "\n",
    "<img src=\"../fig/output_state.png\" width=\"600\" align=\"left\"/>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Code \n",
    "\n",
    "\n",
    "```python\n",
    "def LSTM(prev_ct, prev_ht, X_t):\n",
    "\n",
    "    combine = [prev_ct. prev_ht, X_t] \n",
    "    ft = forget_valve(combine)\n",
    "\n",
    "    # G_t\n",
    "    candidate = new_memory_valve(combine)\n",
    "\n",
    "    it = input_layer(combine)\n",
    "\n",
    "    C_t = prev_ct * f_t + candidate * it\n",
    "\n",
    "    ot = output_layer(combine)\n",
    "\n",
    "    ht = ot * tanh(Ct)\n",
    "\n",
    "    return ht, Ct\n",
    "\n",
    "\n",
    "ct = [0, 0, 0]\n",
    "ht = [0, 0, 0]\n",
    "\n",
    "for Xt in inputs:\n",
    "    ct_, ht = LSTM(ct,ht, Xt)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-timeseries]",
   "language": "python",
   "name": "conda-env-tensorflow-timeseries-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
